//===----------------------------------------------------------------------===//
//
// Part of libcu++, the C++ Standard Library for your entire system,
// under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
// SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES.
//
//===----------------------------------------------------------------------===//

// This is a autogenerated file, we want to ensure that it contains exactly the contentes we want to generate
// clang-format off

#ifndef _LIBCUDACXX__FLOAT_GENERATED
#define _LIBCUDACXX__FLOAT_GENERATED

#include <cuda/std/detail/__config>

#if defined(_CCCL_IMPLICIT_SYSTEM_HEADER_GCC)
#  pragma GCC system_header
#elif defined(_CCCL_IMPLICIT_SYSTEM_HEADER_CLANG)
#  pragma clang system_header
#elif defined(_CCCL_IMPLICIT_SYSTEM_HEADER_MSVC)
#  pragma system_header
#endif // no system header

#include <cuda/__float/arithmetic.h>
#include <cuda/__float/helpers.h>
#include <cuda/__float/representation.h>

_LIBCUDACXX_BEGIN_NAMESPACE_CUDA

using __f16_repr = __float_repr<2, 5, 10>;
using __bf16_repr = __float_repr<2, 8, 7>;

static _LIBCUDACXX_INLINE_VISIBILITY __f16_repr __cuda_float_add(__f16_repr __lhs, __f16_repr __rhs) {
  NV_IF_ELSE_TARGET(NV_IS_DEVICE, (
    using _Tp = __uint_least_t<sizeof(__f16_repr) * CHAR_BIT>;
    _Tp __lhs_, __rhs_, __result;
    __aligned_memcpy(__lhs_, __lhs);
    __aligned_memcpy(__rhs_, __rhs);
    asm volatile ("add.f16 %0, %1, %2;" : "=h"(__result) : "h"(__lhs_), "h"(__rhs_));
    __f16_repr __ret;
    __aligned_memcpy(__ret, __result);
    return __ret;
  ), (
     return __cuda_float_add(__lhs, __rhs, false);
  ));
}

static _LIBCUDACXX_INLINE_VISIBILITY __f16_repr __cuda_float_sub(__f16_repr __lhs, __f16_repr __rhs) {
  NV_IF_ELSE_TARGET(NV_IS_DEVICE, (
    using _Tp = __uint_least_t<sizeof(__f16_repr) * CHAR_BIT>;
    _Tp __lhs_, __rhs_, __result;
    __aligned_memcpy(__lhs_, __lhs);
    __aligned_memcpy(__rhs_, __rhs);
    asm volatile ("sub.f16 %0, %1, %2;" : "=h"(__result) : "h"(__lhs_), "h"(__rhs_));
    __f16_repr __ret;
    __aligned_memcpy(__ret, __result);
    return __ret;
  ), (
     return __cuda_float_sub(__lhs, __rhs, false);
  ));
}

static _LIBCUDACXX_INLINE_VISIBILITY __f16_repr __cuda_float_mul(__f16_repr __lhs, __f16_repr __rhs) {
  NV_IF_ELSE_TARGET(NV_IS_DEVICE, (
    using _Tp = __uint_least_t<sizeof(__f16_repr) * CHAR_BIT>;
    _Tp __lhs_, __rhs_, __result;
    __aligned_memcpy(__lhs_, __lhs);
    __aligned_memcpy(__rhs_, __rhs);
    asm volatile ("mul.f16 %0, %1, %2;" : "=h"(__result) : "h"(__lhs_), "h"(__rhs_));
    __f16_repr __ret;
    __aligned_memcpy(__ret, __result);
    return __ret;
  ), (
     return __cuda_float_mul(__lhs, __rhs, false);
  ));
}

static _LIBCUDACXX_INLINE_VISIBILITY __bf16_repr __cuda_float_add(__bf16_repr __lhs, __bf16_repr __rhs) {
  NV_IF_ELSE_TARGET(NV_IS_DEVICE, (
    using _Tp = __uint_least_t<sizeof(__bf16_repr) * CHAR_BIT>;
    _Tp __lhs_, __rhs_, __result;
    __aligned_memcpy(__lhs_, __lhs);
    __aligned_memcpy(__rhs_, __rhs);
    asm volatile ("add.bf16 %0, %1, %2;" : "=h"(__result) : "h"(__lhs_), "h"(__rhs_));
    __bf16_repr __ret;
    __aligned_memcpy(__ret, __result);
    return __ret;
  ), (
     return __cuda_float_add(__lhs, __rhs, false);
  ));
}

static _LIBCUDACXX_INLINE_VISIBILITY __bf16_repr __cuda_float_sub(__bf16_repr __lhs, __bf16_repr __rhs) {
  NV_IF_ELSE_TARGET(NV_IS_DEVICE, (
    using _Tp = __uint_least_t<sizeof(__bf16_repr) * CHAR_BIT>;
    _Tp __lhs_, __rhs_, __result;
    __aligned_memcpy(__lhs_, __lhs);
    __aligned_memcpy(__rhs_, __rhs);
    asm volatile ("sub.bf16 %0, %1, %2;" : "=h"(__result) : "h"(__lhs_), "h"(__rhs_));
    __bf16_repr __ret;
    __aligned_memcpy(__ret, __result);
    return __ret;
  ), (
     return __cuda_float_sub(__lhs, __rhs, false);
  ));
}

static _LIBCUDACXX_INLINE_VISIBILITY __bf16_repr __cuda_float_mul(__bf16_repr __lhs, __bf16_repr __rhs) {
  NV_IF_ELSE_TARGET(NV_IS_DEVICE, (
    using _Tp = __uint_least_t<sizeof(__bf16_repr) * CHAR_BIT>;
    _Tp __lhs_, __rhs_, __result;
    __aligned_memcpy(__lhs_, __lhs);
    __aligned_memcpy(__rhs_, __rhs);
    asm volatile ("mul.bf16 %0, %1, %2;" : "=h"(__result) : "h"(__lhs_), "h"(__rhs_));
    __bf16_repr __ret;
    __aligned_memcpy(__ret, __result);
    return __ret;
  ), (
     return __cuda_float_mul(__lhs, __rhs, false);
  ));
}

_LIBCUDACXX_END_NAMESPACE_CUDA

#endif

// clang-format on
